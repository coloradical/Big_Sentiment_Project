{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "from requests_oauthlib import OAuth1\n",
    "import oauth2 as oauth\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials from local source:\n",
    "reader = csv.reader(open(\"/Users/mchifala/Desktop/ATLS_5412/Credentials.csv\"))\n",
    "credentials = {}\n",
    "for line in reader:\n",
    "    credentials[line[0]] = line[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Reddit credentials:\n",
    "def getRedditHeaders():\n",
    "    reddit_id = credentials['reddit_id']\n",
    "    reddit_secret = credentials['reddit_secret']\n",
    "    reddit_username = credentials['reddit_username']\n",
    "    reddit_password = credentials['reddit_password']\n",
    "    reddit_user_agent = credentials['reddit_user_agent']\n",
    "\n",
    "    reddit_auth = requests.auth.HTTPBasicAuth(reddit_id, reddit_secret)\n",
    "    reddit_post_data = {'grant_type': 'password', 'username': reddit_username, 'password': reddit_password}\n",
    "    reddit_token_headers = {'User-Agent': reddit_user_agent}\n",
    "    r = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth= reddit_auth, data=reddit_post_data, headers=reddit_token_headers)\n",
    "    #print(r.json())\n",
    "\n",
    "    reddit_token = r.json()['token_type'] + ' ' + r.json()['access_token']\n",
    "    \n",
    "    # Return header dictionary\n",
    "    return {\"Authorization\": reddit_token, \"User-Agent\": reddit_user_agent, 'Content-Type': 'application/json'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests_oauthlib.oauth1_auth.OAuth1 at 0x10a00f208>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Twitter credentials:\n",
    "def getTwitterAuth():\n",
    "    twitter_key = credentials['twitter_key']\n",
    "    twitter_secret =  credentials['twitter_secret']\n",
    "    twitter_token = credentials['twitter_token']\n",
    "    twitter_secret_token = credentials['twitter_secret_token']\n",
    "    \n",
    "    #Return authorization object\n",
    "    return OAuth1(twitter_key, twitter_secret, twitter_token, twitter_secret_token)\n",
    "\n",
    "getTwitterAuth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all available Twitter trends:\n",
    "twitter_auth = getTwitterAuth()\n",
    "r = requests.get('https://api.twitter.com/1.1/trends/available.json', auth = twitter_auth)\n",
    "#print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2972, 3369, 3444, 3534, 4118, 8676, 8775, 9807, 12723]\n"
     ]
    }
   ],
   "source": [
    "# Extract the location ID codes for trends:\n",
    "world_codes = []\n",
    "for i in range(0,len(r.json())):\n",
    "    world_codes.append(r.json()[i]['woeid'])\n",
    "\n",
    "test_codes = world_codes[0:10]\n",
    "print(test_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#UFC235\n",
      "#nitiasa\n",
      "#LulaGigante\n",
      "#Hilbet10TLVeriyor\n",
      "#UFC235\n",
      "Diego Sanchez\n",
      "#VWFC\n",
      "#1stHabsGoal\n",
      "#TavaresDayTO\n",
      "#TFCLive\n",
      "Boucher\n",
      "Khloe\n",
      "#UFC235\n",
      "#VWFC\n",
      "#1stHabsGoal\n",
      "#ThankYouIggy\n",
      "Montr√©al\n",
      "#UFC235\n",
      "#VWFC\n",
      "#1stHabsGoal\n",
      "#TavaresDayTO\n",
      "#EatEnergize\n",
      "Don Cherry\n",
      "#UFC235\n",
      "#Oilers\n",
      "#UFC235\n",
      "#VWFC\n",
      "#1stHabsGoal\n",
      "#UFC235\n",
      "#VWFC\n",
      "#1stHabsGoal\n",
      "#ThankYouIggy\n",
      "#UFC235\n",
      "#VWFC\n",
      "#1stHabsGoal\n",
      "#EatEnergize\n",
      "#avfc\n",
      "Brilliant\n",
      "Leeds\n",
      "Absolutely\n"
     ]
    }
   ],
   "source": [
    "#Search each location ID for its specific trends:\n",
    "topics = []\n",
    "for code in test_codes:\n",
    "    r2 = requests.get('https://api.twitter.com/1.1/trends/place.json?id='+str(code), auth = twitter_auth)\n",
    "    for i in range(0,len(r2.json()[0])):\n",
    "        print(r2.json()[0]['trends'][i]['name'])\n",
    "        topics.append((r2.json()[0]['trends'][i]['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Reddit queries for trends and upload to Elasticsearch:\n",
    "reddit_headers = getRedditHeaders()\n",
    "for query in ['#1erButCH']: #set(topics):\n",
    "    index = 'test_02_23_19'\n",
    "    start = time.time()\n",
    "    r_get = requests.get('https://oauth.reddit.com/r/all/search?q='+query+'&t=day&limit=2', headers = reddit_headers)\n",
    "    r_get.headers['content-type']\n",
    "    list = r_get.json()['data']['children']\n",
    "    end = time.time()\n",
    "\n",
    "    data = []\n",
    "    fields_to_keep = ['id','title', 'author', 'score', 'subreddit', 'num_comments', 'num_crossposts', 'created_utc']\n",
    "    for value in list:\n",
    "        print('hi'+ value)\n",
    "        temp = {k: value['data'][k] for k in fields_to_keep}\n",
    "        temp.update({'hashtag': query})\n",
    "        data.append({'index': {\"_index\": index, '_type': '_doc'}, '_id': 'r_'+ temp['id'] })\n",
    "        del temp['id']\n",
    "        data.append(temp)\n",
    "        \n",
    "    data_to_post = '\\n'.join(json.dumps(d) for d in data)  \n",
    "    #print(data_to_post)\n",
    "    #r_post = requests.post('http://localhost:9200/'+index+'/_bulk',headers = headers, data=data_to_post+'\\n')\n",
    "    \n",
    "    #print(r_post.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"index\": {\"_index\": \"test5\", \"_type\": \"type1\", \"_id\": \"1\"}}\n",
      "{\"field1\": \"value1\"}\n",
      "{\"delete\": {\"_index\": \"test5\", \"_type\": \"type1\", \"_id\": \"2\"}}\n",
      "{\"create\": {\"_index\": \"test5\", \"_type\": \"type1\", \"_id\": \"3\"}}\n",
      "{\"field1\": \"value3\"}\n",
      "{\"update\": {\"_id\": \"1\", \"_type\": \"type1\", \"_index\": \"test5\"}}\n",
      "{\"doc\": {\"field2\": \"value2\"}}\n"
     ]
    }
   ],
   "source": [
    "# This is a working example: make sure the indexes match\n",
    "import json\n",
    "data = [\n",
    "    { \"index\" : { \"_index\" : \"test5\", \"_type\" : \"type1\", \"_id\" : \"1\" } },\n",
    "    { \"field1\" : \"value1\" },\n",
    "    { \"delete\" : { \"_index\" : \"test5\", \"_type\" : \"type1\", \"_id\" : \"2\" }, },\n",
    "    { \"create\" : { \"_index\" : \"test5\", \"_type\" : \"type1\", \"_id\" : \"3\" }, },\n",
    "    { \"field1\" : \"value3\" },\n",
    "    { \"update\" : {\"_id\" : \"1\", \"_type\" : \"type1\", \"_index\" : \"test5\"} },\n",
    "    { \"doc\" : {\"field2\" : \"value2\"} }\n",
    "]\n",
    "\n",
    "data_to_post = '\\n'.join(json.dumps(d) for d in data)\n",
    "print(data_to_post)\n",
    "#r = requests.post('http://localhost:9200/test5/_bulk', headers = headers, data=data_to_post + '\\n')\n",
    "#print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communicating with Elastic Search:\n",
    "#r = requests.get('http://localhost:9200/twitter/_doc/TQqw02gBSrHX7MwTUvS-', headers = {'User-agent': 'mchifala'})\n",
    "requests.post('http://34.73.60.209:9200/hi_yash/_doc/',  json = {\"user\" : \"kimchy\",\n",
    "    \"post_date\" : \"2019-01-28T14:12:12\",\n",
    "    \"message\" : \"Hi Yash\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
