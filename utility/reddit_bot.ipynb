{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "from requests_oauthlib import OAuth1\n",
    "import oauth2 as oauth\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import csv\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from kafka import KafkaProducer\n",
    "from json import dumps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials from local source:\n",
    "reader = csv.reader(open(\"/Users/mchifala/Desktop/ATLS_5412/Credentials.csv\"))\n",
    "credentials = {}\n",
    "for line in reader:\n",
    "    credentials[line[0]] = line[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Reddit credentials for API:\n",
    "def getRedditHeaders():\n",
    "    reddit_id = credentials['reddit_id']\n",
    "    reddit_secret = credentials['reddit_secret']\n",
    "    reddit_username = credentials['reddit_username']\n",
    "    reddit_password = credentials['reddit_password']\n",
    "    reddit_user_agent = credentials['reddit_user_agent']\n",
    "\n",
    "    reddit_auth = requests.auth.HTTPBasicAuth(reddit_id, reddit_secret)\n",
    "    reddit_post_data = {'grant_type': 'password', 'username': reddit_username, 'password': reddit_password}\n",
    "    reddit_token_headers = {'User-Agent': reddit_user_agent}\n",
    "    r_auth = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth= reddit_auth, data=reddit_post_data, headers=reddit_token_headers)\n",
    "\n",
    "    reddit_token = r_auth.json()['token_type'] + ' ' + r_auth.json()['access_token']\n",
    "    \n",
    "    # Return header dictionary\n",
    "    return {\"Authorization\": reddit_token, \"User-Agent\": reddit_user_agent, 'Content-Type': 'application/json'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests_oauthlib.oauth1_auth.OAuth1 at 0x10870cfd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Twitter credentials:\n",
    "def getTwitterAuth():\n",
    "    twitter_key = credentials['twitter_key']\n",
    "    twitter_secret =  credentials['twitter_secret']\n",
    "    twitter_token = credentials['twitter_token']\n",
    "    twitter_secret_token = credentials['twitter_secret_token']\n",
    "    \n",
    "    #Return authorization object\n",
    "    return OAuth1(twitter_key, twitter_secret, twitter_token, twitter_secret_token)\n",
    "\n",
    "getTwitterAuth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all available Twitter trends:\n",
    "twitter_auth = getTwitterAuth()\n",
    "r_trends = requests.get('https://api.twitter.com/1.1/trends/available.json', auth = twitter_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26042, 26062, 26734, 28218, 28869, 30079, 30720, 32185, 32452, 32566]\n"
     ]
    }
   ],
   "source": [
    "# Extract the location ID codes for trends:\n",
    "world_codes = []\n",
    "for i in range(0,len(r_trends.json())):\n",
    "    world_codes.append(r_trends.json()[i]['woeid'])\n",
    "\n",
    "test_codes = world_codes[20:30]\n",
    "print(test_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#MuellerReport\n",
      "#NAPARS\n",
      "Dave East\n",
      "Ontas\n",
      "#CELLYSZN\n",
      "Happy Easter\n",
      "#MuellerReport\n",
      "#EasterWeekend\n",
      "Happy Easter\n",
      "#MuellerReport\n",
      "#ocsbLent\n",
      "Stations of the Cross\n",
      "#CELLYSZN\n",
      "Happy Easter\n",
      "#MuellerReport\n",
      "#EasterWeekend\n",
      "Québec\n",
      "#CELLYSZN\n",
      "Happy Easter\n",
      "#MuellerReport\n",
      "Toronto Public Health\n",
      "Happy Easter\n",
      "endangered species act\n",
      "#longweekend\n",
      "Albertans\n",
      "Calgary\n",
      "Kenney\n",
      "#CELLYSZN\n",
      "#CELLYSZN\n",
      "Happy Easter\n",
      "#MuellerReport\n",
      "#EasterWeekend\n",
      "#CELLYSZN\n",
      "Happy Easter\n",
      "#MuellerReport\n",
      "#EasterWeekend\n",
      "#COYS\n",
      "Spurs\n",
      "Easter\n",
      "Liverpool\n",
      "#Blackpool\n",
      "Easter\n",
      "#BWFC\n",
      "Napoli\n",
      "Easter\n",
      "Spurs\n",
      "Liverpool\n",
      "#COYS\n",
      "Napoli\n",
      "#NAPARS\n",
      "#ClimateCatastrophe\n",
      "#CHESLA\n",
      "Easter\n",
      "Liverpool\n",
      "Napoli\n",
      "#NAPARS\n",
      "Good Friday\n",
      "#COYS\n",
      "Spurs\n",
      "Easter\n",
      "#COYG\n",
      "#EasterWeekend\n",
      "#MCITOT\n",
      "Happy Easter\n",
      "#Easter\n",
      "Happy Easter\n",
      "Spurs\n",
      "HTFP\n",
      "Napoli\n",
      "#NAPARS\n",
      "#ClimateCatastrophe\n",
      "#CHESLA\n",
      "Napoli\n",
      "Easter\n",
      "Spurs\n",
      "Alex McLeish\n",
      "Napoli\n",
      "#NAPARS\n",
      "#ClimateCatastrophe\n",
      "#CHESLA\n",
      "Napoli\n",
      "#NAPARS\n",
      "#ClimateCatastrophe\n",
      "#CHESLA\n",
      "#COYS\n",
      "Liverpool\n",
      "Easter\n",
      "Napoli\n",
      "Easter\n",
      "Barca\n",
      "Ajax\n",
      "Napoli\n",
      "Napoli\n",
      "#NAPARS\n",
      "#ClimateCatastrophe\n",
      "#CHESLA\n",
      "Easter\n",
      "Napoli\n",
      "#NAPARS\n",
      "#ClimateCatastrophe\n",
      "Napoli\n",
      "#NAPARS\n",
      "#ClimateCatastrophe\n",
      "#CHESLA\n",
      "Napoli\n",
      "#NAPARS\n",
      "#ClimateCatastrophe\n",
      "#CHESLA\n",
      "Napoli\n",
      "#NAPARS\n",
      "#ClimateCatastrophe\n",
      "#CHESLA\n",
      "Easter\n",
      "Spurs\n",
      "#COYS\n",
      "#MCITOT\n",
      "Napoli\n",
      "#NAPARS\n",
      "#ClimateCatastrophe\n",
      "#CHESLA\n",
      "Easter\n",
      "Sheffield\n",
      "Napoli\n",
      "#NAPARS\n",
      "Spurs\n",
      "Jesus\n",
      "Easter\n",
      "Ajax\n",
      "#OurBloodIsBlack\n",
      "Napoli\n",
      "#NAPARS\n",
      "#ClimateCatastrophe\n",
      "Liverpool\n",
      "Easter\n",
      "Ajax\n",
      "United\n",
      "#Belfasthour\n",
      "Easter\n",
      "#MCITOT\n",
      "Spurs\n",
      "#SemanaSanta2019\n",
      "Semana Santa\n",
      "Jesús\n",
      "Messi\n",
      "#JuevesSanto\n",
      "Mario Estrada\n",
      "Miami\n",
      "EEUU\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "Ontas\n",
      "Uber\n",
      "#FelizJueves\n",
      "#JuevesSanto\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "Ontas\n",
      "Uber\n",
      "#SemanaSanta\n",
      "Constitución\n",
      "#CNnoticia\n",
      "#CNencuesta\n",
      "#CDMX\n",
      "Semana Santa\n",
      "Uber\n",
      "IMSS\n",
      "Edomex\n",
      "Germán Martínez\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "Ontas\n",
      "Uber\n",
      "#FelizJueves\n",
      "#SemanaSanta\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "Ontas\n",
      "Uber\n",
      "#FelizJueves\n",
      "#SemanaSanta\n",
      "Uber\n",
      "Semana Santa\n",
      "Presidente\n",
      "Ojalá\n",
      "Presidente\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "#Ontas\n",
      "Uber\n",
      "\"Ontas\"\n",
      "Maravillas\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "Uber\n",
      "#FelizJueves\n",
      "#SemanaSanta\n",
      "Liverpool\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "Ontas\n",
      "Uber\n",
      "#JuevesSanto\n",
      "#EuropaLeague\n",
      "#ColapsoPRO\n",
      "Wall Street\n",
      "Mejor DT\n",
      "Ontas\n",
      "#AlEstadioconADN\n",
      "#VerdadesOcultas\n",
      "Semana Santa\n",
      "#eresunenfermo\n",
      "#botsdekast\n",
      "Jason Momoa\n",
      "J. A. Kast\n",
      "#AlEstadioconADN\n",
      "#botsdekast\n",
      "Jason Momoa\n",
      "J. A. Kast\n",
      "#AlEstadioconADN\n",
      "#LaLigaxRCN\n",
      "#JuevesSanto\n",
      "#PorQuePiensoQueFuePesimo\n",
      "Yesus Cabrera\n",
      "#LaLigaxRCN\n",
      "#JuevesSanto\n",
      "#PorQuePiensoQueFuePesimo\n",
      "Yesus Cabrera\n",
      "Semana Santa\n",
      "Petro\n",
      "Totalmente\n",
      "Venezuela\n",
      "Petro\n",
      "Messi\n",
      "Venezuela\n",
      "Semana Santa\n",
      "#ViajaSeguroEC\n",
      "Interpol\n",
      "Patiño\n",
      "#LasVacunasFuncionan\n",
      "Patiño\n",
      "#ViajaSeguroEC\n",
      "Flavio Palomo\n",
      "Interpol\n",
      "#18Abr\n",
      "#FestivalSemanaSanta2019\n",
      "Annika Rothstein\n",
      "Reina del Clap\n",
      "#18Abr\n",
      "#JuevesSanto\n",
      "alan garcía\n",
      "Odebrecht\n",
      "#18Abr\n",
      "Alan García\n",
      "Banco Central de Venezuela\n",
      "#FestivalSemanaSanta2019\n",
      "#18Abr\n",
      "Alan García\n",
      "Odebrecht\n",
      "#JuevesSanto\n",
      "#18Abr\n",
      "#FestivalSemanaSanta2019\n",
      "Annika Rothstein\n",
      "Reina del Clap\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3549d202e7ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworld_codes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mr_trends2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://api.twitter.com/1.1/trends/place.json?id='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter_auth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_trends2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_trends2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trends'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_trends2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trends'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Search each location ID for its specific trends:\n",
    "topics = []\n",
    "for code in world_codes:\n",
    "    r_trends2 = requests.get('https://api.twitter.com/1.1/trends/place.json?id='+str(code), auth = twitter_auth)\n",
    "    for i in range(0,len(r_trends2.json()[0])):\n",
    "        print(r_trends2.json()[0]['trends'][i]['name'])\n",
    "        topics.append((r_trends2.json()[0]['trends'][i]['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to control the number of calls to Reddit API; sleep when limit is exceeded\n",
    "def redditSearch(reddit_headers, elastic_headers, elastic_index, ratelimit_calls, ratelimit_reset, trends):\n",
    "    for query in set(trends):\n",
    "        if (int(float(ratelimit_calls)) > 0):\n",
    "            #print('Before call')\n",
    "            #print(ratelimit_calls, ratelimit_reset)\n",
    "            ratelimit_calls, ratelimit_reset = redditQuery(reddit_headers, elastic_headers, elastic_index, ratelimit_calls, ratelimit_reset, query)  \n",
    "            #print('After call')\n",
    "            #print(ratelimit_calls, ratelimit_reset)\n",
    "        else:\n",
    "            print('Sleep')\n",
    "            time.sleep(int(ratelimit_reset))\n",
    "            ratelimit_calls, ratelimit_reset = redditQuery(reddit_headers, elastic_headers, elastic_index, ratelimit_calls, ratelimit_reset, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original function for publishing directly to Elasticsearch\n",
    "def redditQuery(reddit_headers,elastic_headers,elastic_index, ratelimit_calls, ratelimit_reset, query):\n",
    "    r_get = requests.get('https://oauth.reddit.com/r/all/search?q='+query+'&t=day&limit=25', headers = reddit_headers)\n",
    "    posts = r_get.json()['data']['children']\n",
    "    \n",
    "    data = []\n",
    "    fields_to_keep = ['id','title', 'author', 'score', 'subreddit', 'num_comments', 'created_utc', 'url']\n",
    "    for post in posts:\n",
    "        temp = {k: post['data'][k] for k in fields_to_keep}\n",
    "        temp.update({'hashtag': query, 'source': 'reddit'})\n",
    "        temp['post_date'] = datetime.isoformat(datetime.utcfromtimestamp(temp['created_utc']))\n",
    "        temp['upvotes'] = temp['score']\n",
    "        data.append({'index': {\"_index\": elastic_index, '_type': '_doc', '_id': 'r_'+ temp['id'] }})\n",
    "        url_split = temp['url'].split('.')[-1]\n",
    "        if (url_split != \"jpg\" and url_split != \"png\"):\n",
    "            del temp['url']\n",
    "        del temp['id']\n",
    "        del temp['created_utc']\n",
    "        del temp['score']\n",
    "        data.append(temp)\n",
    "\n",
    "    if data:\n",
    "        data_to_post = '\\n'.join(json.dumps(d) for d in data)\n",
    "        print(data_to_post)\n",
    "        r_post = requests.post('http://34.73.60.209:9200/'+elastic_index+'/_bulk', headers = elastic_headers, data=data_to_post+'\\n')\n",
    "        print(r_post.text)\n",
    "    \n",
    "    print(r_get.headers['x-ratelimit-remaining'], r_get.headers['x-ratelimit-reset'])\n",
    "    \n",
    "    return r_get.headers['x-ratelimit-remaining'], r_get.headers['x-ratelimit-reset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('598.0', '172')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revised function for publishing to Kakfa-Spark-Elasticsearch pipeline\n",
    "def redditQuery(reddit_headers,elastic_headers,elastic_index, ratelimit_calls, ratelimit_reset, query, producer, topic):\n",
    "    r_get = requests.get('https://oauth.reddit.com/r/all/search?q='+query+'&t=day&limit=25', headers = reddit_headers)\n",
    "    posts = r_get.json()['data']['children']\n",
    "    \n",
    "    fields_to_keep = ['id','title', 'author', 'score', 'subreddit', 'num_comments', 'created_utc', 'url']\n",
    "    for post in posts:\n",
    "        \n",
    "        data = {k: post['data'][k] for k in fields_to_keep}\n",
    "        time = datetime.isoformat(datetime.utcfromtimestamp(data['created_utc']))\n",
    "        data.update({'hashtag': query, 'source': 'reddit', 'id': 'r_'+ data['id'], 'upvotes': data['score'], 'post_date': time })    \n",
    "        url_split = data['url'].split('.')[-1]\n",
    "        if (url_split != \"jpg\" and url_split != \"png\"):\n",
    "            del data['url']\n",
    "            \n",
    "        del data['created_utc']\n",
    "        del data['score']\n",
    "        \n",
    "        producer.send(topic, value=data)\n",
    "\n",
    "    return r_get.headers['x-ratelimit-remaining'], r_get.headers['x-ratelimit-reset']\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=['35.232.117.118:9092'], \n",
    "                        value_serializer=lambda x: dumps(x).encode('utf-8'),\n",
    "                        api_version = (0,10))\n",
    "topic = 'trending'\n",
    "redditQuery(reddit_headers, elastic_headers, elastic_index, ratelimit_calls, ratelimit_reset, query, producer, topic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Enviroment for Rate Limits:\n",
    "reddit_headers = getRedditHeaders()\n",
    "elastic_headers = {'Content-Type':'application/json'}\n",
    "elastic_index = 'hi_yash2'\n",
    "\n",
    "r_get = requests.get('https://oauth.reddit.com', headers = reddit_headers)\n",
    "ratelimit_calls =  r_get.headers['x-ratelimit-remaining']\n",
    "ratelimit_reset = r_get.headers['x-ratelimit-reset']\n",
    "\n",
    "redditSearch(reddit_headers, elastic_headers, elastic_index, ratelimit_calls, ratelimit_reset, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a working example of bulk uploads to Elasticsearch: make sure the indexes match\n",
    "data = [\n",
    "    { \"index\" : { \"_index\" : \"test5\", \"_type\" : \"type1\", \"_id\" : \"1\" } },\n",
    "    { \"field1\" : \"value1\" },\n",
    "    { \"delete\" : { \"_index\" : \"test5\", \"_type\" : \"type1\", \"_id\" : \"2\" }, },\n",
    "    { \"create\" : { \"_index\" : \"test5\", \"_type\" : \"type1\", \"_id\" : \"3\" }, },\n",
    "    { \"field1\" : \"value3\" },\n",
    "    { \"update\" : {\"_id\" : \"1\", \"_type\" : \"type1\", \"_index\" : \"test5\"} },\n",
    "    { \"doc\" : {\"field2\" : \"value2\"} }\n",
    "]\n",
    "\n",
    "data_to_post = '\\n'.join(json.dumps(d) for d in data)\n",
    "print(data_to_post)\n",
    "r = requests.post('http://34.73.60.209:9200/test5/_bulk', headers = headers, data=data_to_post + '\\n')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 3, 10, 21, 19, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a working example of a single record uplaod to Elasticsearch:\n",
    "requests.post('http://34.73.60.209:9200/hi_yash/_doc/',  json = {\"user\" : \"mchifala\",\n",
    "  \"post_date\" : \"2019-03-10T15:41:12\",\n",
    "  \"message\" : \"Hi Yash\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
